{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Set the directory path containing the PDFs\n",
    "pdf_directory = \"C:/Users/AbhinavKasubojula/OneDrive - Kenall Inc/Desktop/code/docs/\"\n",
    "\n",
    "# Ollama API endpoint and headers\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "\n",
    "def compute_embeddings(documents):\n",
    "    model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "    embeddings = model.encode(documents, convert_to_tensor=False)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        \n",
    "        images = convert_from_path(pdf_path)\n",
    "        \n",
    "        # Extract text from each image using Tesseract OCR\n",
    "        text = \"\"\n",
    "        for page_num, image in enumerate(images):\n",
    "            text += pytesseract.image_to_string(image)  # OCR on the image\n",
    "            print(f\"Extracted text from page {page_num + 1} of {os.path.basename(pdf_path)}...\")\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to summarize text using the Ollama API\n",
    "def summarize_text_with_ollama(text):\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following resume:\n",
    "    - Name\n",
    "    - Role\n",
    "    - Licenses\n",
    "    - working for (company name)\n",
    "    - Qualifications\n",
    "    - Experience\n",
    "    - Projects\n",
    "    - Roles and Responsibilities\n",
    "\n",
    "    Resume Text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    data = {\"model\": \"llama3.2-vision:latest\", \"prompt\": prompt, \"stream\": False}\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    \n",
    "    response_json = response.json()\n",
    "    summary = response_json.get(\"response\", \"No response found\")\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract key points (simple placeholder function for demo)\n",
    "def extract_key_point(summary, key):\n",
    "    # This is a simple way to extract key points, you can refine it based on your summary format\n",
    "    start = summary.find(key)\n",
    "    end = summary.find(\"\\n\", start)\n",
    "    return summary[start:end].strip() if start != -1 else \"\"\n",
    "\n",
    "# Process each PDF file, extract text, summarize, and add data to FAISS\n",
    "def process(sum, file_names):\n",
    "    pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith(\".pdf\")]\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "        print(f\"Processing: {pdf_file}\")\n",
    "        \n",
    "        # Extract text from the PDF using OCR\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if text:\n",
    "            # Summarize the extracted text\n",
    "            summary = summarize_text_with_ollama(text)\n",
    "            filename = extract_key_point(summary, \"Name\")\n",
    "            print(f\"Summary of {filename}: {summary[:100]}...\")  # Limit printed summary lengt\n",
    "        sum.append(summary)\n",
    "        file_names.append(filename)\n",
    "    return(sum,file_names)\n",
    "\n",
    "\n",
    "sum=[]\n",
    "file_names=[]\n",
    "# Run the process\n",
    "sum , file_names= process(sum, file_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import requests\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from weaviate import Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AbhinavKasubojula\\AppData\\Local\\Temp\\ipykernel_21708\\2383575838.py:12: DeprecationWarning: \n",
      "Python client v3 `weaviate.Client(...)` connections and methods are deprecated and will\n",
      "            be removed by 2024-11-30.\n",
      "\n",
      "            Upgrade your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n",
      "                - For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "                - For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n",
      "\n",
      "            If you have to use v3 code, install the v3 client and pin the v3 dependency in your requirements file: `weaviate-client>=3.26.7;<4.0.0`\n",
      "  client = Client(\"http://localhost:8080\")  # Adjust URL if needed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema already exists or error: Create class! Unexpected status code: 422, with response body: {'error': [{'message': 'class name Resume already exists'}]}.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize SentenceTransformer\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Set directory containing PDFs\n",
    "pdf_directory = \"C:/Users/AbhinavKasubojula/OneDrive - Kenall Inc/Desktop/code/docs/\"\n",
    "\n",
    "# Ollama API details\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Initialize Weaviate client\n",
    "client = Client(\"http://localhost:8080\")  # Adjust URL if needed\n",
    "\n",
    "\n",
    "# Define Weaviate schema for resumes\n",
    "resume_schema = {\n",
    "    \"class\": \"Resume\",\n",
    "    \"vectorizer\": \"none\",  # We provide custom embeddings\n",
    "    \"properties\": [\n",
    "        {\"name\": \"name\", \"dataType\": [\"text\"]},\n",
    "        {\"name\": \"role\", \"dataType\": [\"text\"]},\n",
    "        {\"name\": \"licenses\", \"dataType\": [\"text[]\"]},\n",
    "        {\"name\": \"working_for\", \"dataType\": [\"text\"]},\n",
    "        {\"name\": \"qualifications\", \"dataType\": [\"text\"]},\n",
    "        {\"name\": \"experience\", \"dataType\": [\"text\"]},\n",
    "        {\"name\": \"projects\", \"dataType\": [\"text[]\"]},\n",
    "        {\"name\": \"roles_responsibilities\", \"dataType\": [\"text[]\"]},\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Create schema if it doesn't already exist\n",
    "try:\n",
    "    client.schema.create_class(resume_schema)\n",
    "    print(\"Weaviate schema created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Schema already exists or error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compute embeddings for documents\n",
    "def compute_embeddings(documents):\n",
    "    embeddings = model.encode(documents, convert_to_tensor=False)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract text from a PDF using OCR\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path)  # Convert PDF to images\n",
    "        text = \"\"\n",
    "        for page_num, image in enumerate(images):\n",
    "            text += pytesseract.image_to_string(image)  # OCR on the image\n",
    "            print(f\"Extracted text from page {page_num + 1} of {os.path.basename(pdf_path)}...\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summarize text using Ollama API\n",
    "def summarize_text_with_ollama(text):\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following resume:\n",
    "    - Name\n",
    "    - education qualification(you are able to find at name ending (like PE, BA, etc..,))\n",
    "    Variable parameters: \n",
    "    - Role\n",
    "    - Licenses\n",
    "    - working for (company name)\n",
    "    - Qualifications\n",
    "    - Experience\n",
    "    - Projects he/she worked before\n",
    "    - Roles and Responsibilities\n",
    "\n",
    "    Resume Text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    data = {\"model\": \"llama3.2-vision:latest\", \"prompt\": prompt, \"stream\": False}\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    response_json = response.json()\n",
    "    return response_json.get(\"response\", \"No response found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract key points from the summary\n",
    "def extract_key_point(summary, key):\n",
    "    start = summary.find(key)\n",
    "    end = summary.find(\"\\n\", start)\n",
    "    return summary[start:end].strip() if start != -1 else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store data and embeddings in Weaviate\n",
    "def store_in_weaviate(summary, embedding, file_name):\n",
    "    unique_id = f\"{summary['Name']}\"\n",
    "\n",
    "     # Check if the resume already exists\n",
    "    query = client.query.get(\"Resume\", [\"name\", \"education\"]).with_where({\n",
    "        \"path\": [\"unique_id\"],\n",
    "        \"operator\": \"Equal\",\n",
    "        \"valueString\": unique_id\n",
    "    })\n",
    "    results = query.do()\n",
    "\n",
    "    if results[\"data\"][\"Get\"][\"Resume\"]:\n",
    "        print(\"Resume already exists. Skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Store metadata and embedding\n",
    "    client.data_object.create(\n",
    "        {\n",
    "            \"unique_id\": unique_id,\n",
    "            \"name\": summary[\"Name\"],\n",
    "            \"education\": metadata[\"Education\"],\n",
    "            \"role\": metadata[\"Role\"],\n",
    "            \"company\": metadata[\"Company name\"],\n",
    "            \"projects\": metadata[\"Projects\"],\n",
    "            \"roles_responsibilities\": metadata[\"Roles and Responsibilities\"],\n",
    "        },\n",
    "        vector=embedding,\n",
    "        class_name=\"Resume\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: page_1.pdf\n",
      "Extracted text from page 1 of page_1.pdf...\n",
      "Stored Name:** Kris D. Prasad, PE in Weaviate!\n",
      "Processing: page_2.pdf\n",
      "Extracted text from page 1 of page_2.pdf...\n",
      "Stored Name:** Srujan Chikyala, PE (Project Manager) in Weaviate!\n",
      "Processing: page_3.pdf\n",
      "Extracted text from page 1 of page_3.pdf...\n",
      "Stored Name:** Brett Witte, PE in Weaviate!\n",
      "Processing: page_4.pdf\n",
      "Extracted text from page 1 of page_4.pdf...\n",
      "Stored Name:** Robert Morris, AIA, RID DQC Manager in Weaviate!\n",
      "Processing: page_5.pdf\n",
      "Extracted text from page 1 of page_5.pdf...\n",
      "Stored Name:** Robert Gaylord in Weaviate!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process each PDF, extract text, summarize, and store in Weaviate\n",
    "def process():\n",
    "    pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith(\".pdf\")]\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "        print(f\"Processing: {pdf_file}\")\n",
    "        \n",
    "        # Extract text from the PDF\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if text:\n",
    "            # Summarize the extracted text\n",
    "            summary = summarize_text_with_ollama(text)\n",
    "            file_name = extract_key_point(summary, \"Name\")\n",
    "            #print(f\"Summary of {file_name}: {summary[:100]}...\")  # Print first 100 characters of summary\n",
    "            \n",
    "            # Generate vector embedding\n",
    "            embedding = compute_embeddings([summary])[0]\n",
    "            \n",
    "            # Store data in Weaviate\n",
    "            store_in_weaviate(summary, embedding, file_name)\n",
    "\n",
    "# Run the process\n",
    "process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dynamic query-based resume search\n",
    "def search_resumes(query, limit=2):\n",
    "    # Generate query embedding for the dynamic query\n",
    "    query_embedding = compute_embeddings([query])[0]\n",
    "\n",
    "    # Retrieve results from Weaviate\n",
    "    results = client.query.get(\"Resume\", [\"name\", \"role\", \"experience\",\"projects\"]) \\\n",
    "        .with_near_vector({\"vector\": query_embedding}) \\\n",
    "        .with_limit(limit) \\\n",
    "        .do()\n",
    "\n",
    "    # Extract and clean data\n",
    "    resumes = results.get('data', {}).get('Get', {}).get('Resume', [])\n",
    "    return resumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: User query to search for resumes with experience in civil engineering\n",
    "user_query = \"Resumes with experience in civil engineering\"\n",
    "resumes = search_resumes(user_query)\n",
    "\n",
    "# Display results\n",
    "if resumes:\n",
    "    for resume in resumes:\n",
    "        name = resume.get(\"name\", \"Name not available\").strip()\n",
    "        role = resume.get(\"role\", \"Role not available\").strip()\n",
    "        experience = resume.get(\"experience\", \"Experience not available\").strip()\n",
    "        projects = resume.get(\"projects\",\"projects not available\").strip()\n",
    "        print(f\"Name: {name}\\nRole: {role}\\nExperience: {experience}\\n\")\n",
    "else:\n",
    "    print(\"No matching resumes found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
