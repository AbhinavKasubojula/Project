{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AbhinavKasubojula\\OneDrive - Kenall Inc\\Desktop\\code\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\AbhinavKasubojula\\OneDrive - Kenall Inc\\Desktop\\code\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import os\n",
    "import uuid\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_pdf_file(file_path):\n",
    "    try:\n",
    "        pages = convert_from_path(file_path, dpi=300)\n",
    "        extracted_text = []\n",
    "        \n",
    "        for page in pages:\n",
    "            text = pytesseract.image_to_string(page, lang='eng')\n",
    "            extracted_text.append(text)\n",
    "        \n",
    "        # Combine text from all pages\n",
    "        full_text = '\\n'.join(extracted_text)\n",
    "        \n",
    "        return full_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file {file_path}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_doc(directory):\n",
    "    docs = []\n",
    "    try:\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                text = read_pdf_file(file_path)\n",
    "                if text: \n",
    "                    docs.append(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing directory {directory}: {e}\")\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(documents):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(documents, convert_to_tensor=False)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'docs/'\n",
    "documents = read_doc(directory_path)\n",
    "\n",
    "# Compute embeddings\n",
    "v = compute_embeddings(documents)\n",
    "\n",
    "\n",
    "\n",
    "d = len(v[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 384)\n",
      "Metadata saved: ['Doc 1: Synopsis Architect-Engineer (A-E) Services Indefinite Delivery/Indefinite Quantity (IDIQ) Contract For', 'Doc 2: Green underlined is add-in, Red strike through is deleted This', 'Doc 3: = An official website of the United States government Here’s', 'Doc 4: An official website of the United States government Here’s how', 'Doc 5: INDEFINITE DELIVERY CONTRACT (IDC) A-E SERVICES FOR USE WITHIN SOUTHWESTERN', 'Doc 6: C -- ARCHITECT AND ENGINEERING SERVICES W912DQ24R4002 SYNOPSIS Architect-Engineer (A-E)', 'Doc 7: Indefinite-Delivery (MATOC) for Multi-Discipline Miscellaneous Works Design and Other Architect']\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# FAISS index initialization\n",
    "index = faiss.IndexFlatL2(384)  # d = 384\n",
    "\n",
    "# Add your vectors to the index\n",
    "index.add(v)  # v contains the document embeddings\n",
    "\n",
    "storage_directory = r'C:\\Users\\AbhinavKasubojula\\OneDrive - Kenall Inc\\Desktop\\code\\stored_data'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(storage_directory):\n",
    "    os.makedirs(storage_directory)\n",
    "\n",
    "file_metadata = []\n",
    "for i, doc in enumerate(documents):\n",
    "    doc_number = f\"Doc {i + 1}\"\n",
    "    first_few_words = ' '.join(doc.split()[:10])  \n",
    "    file_metadata.append(f\"{doc_number}: {first_few_words}\")\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, os.path.join(storage_directory, 'faiss_index.bin'))\n",
    "\n",
    "# Save document texts\n",
    "with open(os.path.join(storage_directory, 'documents.pkl'), 'wb') as f:\n",
    "    pickle.dump(documents, f)\n",
    "\n",
    "# Optionally save metadata\n",
    "with open(os.path.join(storage_directory, 'metadata.pkl'), 'wb') as f:\n",
    "    pickle.dump(file_metadata, f)\n",
    "\n",
    "embeddings = np.array(v)\n",
    "print(embeddings.shape)\n",
    "\n",
    "print(\"Metadata saved:\", file_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Doc 1: Synopsis Architect-Engineer (A-E) Services Indefinite Delivery/Indefinite Quantity (IDIQ) Contract For', 'Doc 2: Green underlined is add-in, Red strike through is deleted This', 'Doc 3: = An official website of the United States government Here’s', 'Doc 4: An official website of the United States government Here’s how', 'Doc 5: INDEFINITE DELIVERY CONTRACT (IDC) A-E SERVICES FOR USE WITHIN SOUTHWESTERN', 'Doc 6: C -- ARCHITECT AND ENGINEERING SERVICES W912DQ24R4002 SYNOPSIS Architect-Engineer (A-E)', 'Doc 7: Indefinite-Delivery (MATOC) for Multi-Discipline Miscellaneous Works Design and Other Architect']\n"
     ]
    }
   ],
   "source": [
    "print(file_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AbhinavKasubojula\\OneDrive - Kenall Inc\\Desktop\\code\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring directory: docs/\n",
      "New PDF detected: docs/week6.pdf\n",
      "Processing PDF for OCR: docs/week6.pdf\n",
      "After update: len(documents)=8, len(metadata)=8\n",
      "New file 'docs/week6.pdf' successfully processed and added to FAISS.\n",
      "New PDF detected: docs/week7.pdf\n",
      "Processing PDF for OCR: docs/week7.pdf\n",
      "After update: len(documents)=9, len(metadata)=9\n",
      "New file 'docs/week7.pdf' successfully processed and added to FAISS.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define storage directory\n",
    "storage_directory = r'C:\\Users\\AbhinavKasubojula\\OneDrive - Kenall Inc\\Desktop\\code\\stored_data'\n",
    "\n",
    "# Function to process new files and update FAISS, documents, and metadata\n",
    "def process_new_file(file_path):\n",
    "    text = ocr_processor.process_pdf(file_path)\n",
    "    \n",
    "    if text:  # If the text is successfully extracted\n",
    "        # Load the existing FAISS index, documents, and metadata\n",
    "        index_path = os.path.join(storage_directory, 'faiss_index.bin')\n",
    "        documents_path = os.path.join(storage_directory, 'documents.pkl')\n",
    "        metadata_path = os.path.join(storage_directory, 'metadata.pkl')\n",
    "\n",
    "        # Load FAISS index\n",
    "        if os.path.exists(index_path):\n",
    "            index = faiss.read_index(index_path)\n",
    "        else:\n",
    "            index = faiss.IndexFlatL2(384)  # Create a new FAISS index if none exists\n",
    "\n",
    "        # Load existing documents and metadata\n",
    "        try:\n",
    "            with open(documents_path, 'rb') as f:\n",
    "                documents = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            documents = []\n",
    "\n",
    "        try:\n",
    "            with open(metadata_path, 'rb') as f:\n",
    "                metadata = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            metadata = []\n",
    "\n",
    "        # Convert new text to vector embedding\n",
    "        vector = embedding_model.encode([text])[0]\n",
    "\n",
    "        # Append the new vector to the FAISS index\n",
    "        index.add(np.array([vector]).astype('float32'))\n",
    "\n",
    "        # Append the new text to the documents list\n",
    "        documents.append(text)\n",
    "\n",
    "        # Generate and append new metadata (document number and first few words)\n",
    "        doc_number = f\"Doc {len(documents)}\"  # Make sure document number matches documents list\n",
    "        first_few_words = ' '.join(text.split()[:10])\n",
    "        metadata.append(f\"{doc_number}: {first_few_words}...\")\n",
    "\n",
    "        # Debugging: Print length after update\n",
    "        print(f\"After update: len(documents)={len(documents)}, len(metadata)={len(metadata)}\")\n",
    "\n",
    "        # Save the updated FAISS index\n",
    "        faiss.write_index(index, os.path.join(storage_directory, 'faiss_index.bin'))\n",
    "\n",
    "        # Save the updated documents\n",
    "        with open(os.path.join(storage_directory, 'documents.pkl'), 'wb') as f:\n",
    "            pickle.dump(documents, f)\n",
    "\n",
    "        # Save the updated metadata\n",
    "        with open(os.path.join(storage_directory, 'metadata.pkl'), 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "\n",
    "        print(f\"New file '{file_path}' successfully processed and added to FAISS.\")\n",
    "\n",
    "\n",
    "# Initialize components\n",
    "tesseract_path = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "ocr_processor = OCRProcessor(tesseract_path)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Start monitoring\n",
    "directory_to_watch = 'docs/'\n",
    "file_monitor = FileMonitor(directory_to_watch, process_new_file)\n",
    "file_monitor.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
